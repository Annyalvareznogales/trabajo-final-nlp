{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a769f72-5656-43d8-8154-d5ecbf1cdcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98610ef4-8c0e-434b-b96d-60700d582427",
   "metadata": {},
   "source": [
    "# Limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2577ce6-4933-463b-99c5-ddaf29d83dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de limpieza de datos para los datos en inglés y en español\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpia el texto en inglés eliminando enlaces, normalizando a minúsculas, \n",
    "    eliminando caracteres especiales y números (excepto comas y puntos), \n",
    "    reduciendo múltiples espacios en blanco y eliminando saltos de línea.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'https?\\s*:\\s*//\\s*\\S+', '', text)\n",
    "    text = re.sub(r'www\\.?\\s*\\S+', '', text)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)    \n",
    "    text = text.lower()   \n",
    "    # Mantener comas y puntos)\n",
    "    text = re.sub(r'[^a-z\\s,.]', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def clean_text_es(text):\n",
    "    \"\"\"\n",
    "    Limpia el texto en español eliminando enlaces, normalizando a minúsculas, \n",
    "    eliminando caracteres especiales y números (excepto caracteres acentuados, comas y puntos), \n",
    "    reduciendo múltiples espacios en blanco y eliminando saltos de línea.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'https?\\s*:\\s*//\\s*\\S+', '', text)\n",
    "    text = re.sub(r'www\\.?\\s*\\S+', '', text)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = text.lower()   \n",
    "    text = re.sub(r'[^a-záéíóúñü\\s,.]', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171c126-70ce-40fb-8dae-ac6c4825d3c3",
   "metadata": {},
   "source": [
    "## Aplicar a datos de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2378d3-23e2-471e-ad5b-dc0f5a91cc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf.to_csv(r\"data/train/dataset_es_train_cleaned.csv\", index=False)\\n\\ndf = pd.read_json(r\"data/train/dataset_en_train.json\")\\ndf[\\'text\\'] = df[\\'text\\'].apply(lambda text: clean_text(text))\\ndf.head()\\ndf.to_csv(r\"data/train/dataset_en_train_cleaned.csv\", index=False)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_json(r\"data/train/dataset_en_train.json\")\n",
    "df['text'] = df['text'].apply(lambda text: clean_text_en(text))\n",
    "\n",
    "df = pd.read_json(r\"data/train/dataset_es_train.json\")\n",
    "df['text'] = df['text'].apply(lambda text: clean_text_es(text))\n",
    "\n",
    "df = pd.read_json(r\"data/test/dataset_en_test.json\")\n",
    "df['text'] = df['text'].apply(lambda text: clean_text_en(text))\n",
    "\n",
    "df = pd.read_json(r\"data/test/dataset_es_test.json\")\n",
    "df['text'] = df['text'].apply(lambda text: clean_text_es(text))\n",
    "\n",
    "#df.to_csv(r\"data/train/dataset_en_train_cleaned.csv\", index=False)\n",
    "#df.to_csv(r\"data/test/dataset_en_test_cleaned.csv\", index=False)\n",
    "#df.to_csv(r\"data/train/dataset_es_train_cleaned.csv\", index=False)\n",
    "#df.to_csv(r\"data/test/dataset_es_test_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863dd3c-a92f-4bef-b243-c41439f09e22",
   "metadata": {},
   "source": [
    "# Aumento de Datos con back-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22455f-f956-42f9-86a3-fcee04b3b9b5",
   "metadata": {},
   "source": [
    "## Aumento datos de entrenamiento español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253b2b6-4555-440a-9e2a-d2fd01e309f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el DataFrame\n",
    "\n",
    "df=pd.read_csv('data/train/dataset_es_train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84c80e-9c6b-450d-98bc-bf3ab311a4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Procesando filas:  12%|█▏        | 471/4000 [46:49<5:35:25,  5.70s/it] "
     ]
    }
   ],
   "source": [
    "# Cargar los modelos y tokenizadores de traducción\n",
    "model_name_es_en = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "model_name_en_es = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "tokenizer_es_en = MarianTokenizer.from_pretrained(model_name_es_en)\n",
    "model_es_en = MarianMTModel.from_pretrained(model_name_es_en)\n",
    "\n",
    "tokenizer_en_es = MarianTokenizer.from_pretrained(model_name_en_es)\n",
    "model_en_es = MarianMTModel.from_pretrained(model_name_en_es)\n",
    "\n",
    "def translate_text(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "new_rows = []\n",
    "for i, fila in tqdm(df.iterrows(), total=df.shape[0], desc=\"Procesando filas\"):\n",
    "    # Traducir de español a inglés\n",
    "    text_en = translate_text(fila['text'], tokenizer_es_en, model_es_en)\n",
    "    # Traducir de vuelta de inglés a español\n",
    "    text_es_back = translate_text(text_en, tokenizer_en_es, model_en_es)\n",
    "\n",
    "    # Generar un nuevo ID aleatorio para nuevos datos y mantener originales\n",
    "    id_new = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n",
    "    new_row = {'id': id_new, 'text': text_es_back, 'category': fila['category']}\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "# Concatenar el DataFrame original con el nuevo DataFrame\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# Guardar el DataFrame resultante \n",
    "df.to_csv('data/train/dataset_es_train_augmented.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55385c4c-8aeb-4470-a352-d41647b62375",
   "metadata": {},
   "source": [
    "## Aumento de datos inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979bfa9-7fe6-4e8c-88c5-fa348a6c4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el DataFrame\n",
    "df = pd.read_csv(\"data/train/dataset_en_train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8bc57-2c82-400c-aaab-d21ae9b5dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los modelos y tokenizadores de traducción\n",
    "model_name_es_en = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "model_name_en_es = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "\n",
    "tokenizer_es_en = MarianTokenizer.from_pretrained(model_name_es_en)\n",
    "model_es_en = MarianMTModel.from_pretrained(model_name_es_en)\n",
    "\n",
    "tokenizer_en_es = MarianTokenizer.from_pretrained(model_name_en_es)\n",
    "model_en_es = MarianMTModel.from_pretrained(model_name_en_es)\n",
    "\n",
    "def translate_text(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "new_rows = []\n",
    "for i, fila in tqdm(df.iterrows(), total=df.shape[0], desc=\"Procesando filas\"):\n",
    "    # Traducir de inglés a alemán\n",
    "    text_en = translate_text(fila['text'], tokenizer_es_en, model_es_en)\n",
    "    # Traducir de vuelta de alemán a inglés\n",
    "    text_es_back = translate_text(text_en, tokenizer_en_es, model_en_es)\n",
    "    id_new = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n",
    "    new_row = {'id': id_new, 'text': text_es_back, 'category': fila['category']}\n",
    "    new_rows.append(new_row)\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Concatenar el DataFrame original con el nuevo DataFrame\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# Guardar el DataFrame resultante como augmented\n",
    "df.to_csv('data/train/dataset_en_train_augmented.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
